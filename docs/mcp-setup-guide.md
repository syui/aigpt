# MCP Server ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰
Claude Code + ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆç’°å¢ƒ

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv mcp-env
mcp-env\Scripts\activate  # Windows
# source mcp-env/bin/activate  # Linux/Mac

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install mcp requests pathlib asyncio
```

### 2. Ollamaã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆhttps://ollama.comï¼‰
# Windows: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# Linux: curl -fsSL https://ollama.com/install.sh | sh

# Qwen2.5-Coderãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull qwen2.5-coder:14b-instruct-q4_K_M

# Ollamaã‚µãƒ¼ãƒãƒ¼èµ·å‹•ç¢ºèª
ollama serve
```

### 3. Claude Desktopè¨­å®š

#### claude_desktop_config.json ã®ä½œæˆ
**Windows**: `%APPDATA%\Claude\claude_desktop_config.json`
**macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
**Linux**: `~/.config/claude/claude_desktop_config.json`

```json
{
  "mcpServers": {
    "local-llm": {
      "command": "python",
      "args": ["/path/to/your/local_llm_mcp_server.py"],
      "env": {
        "OLLAMA_URL": "http://localhost:11434",
        "DEFAULT_MODEL": "qwen2.5-coder:14b-instruct-q4_K_M"
      }
    }
  }
}
```

### 4. Claude Codeè¨­å®š

```bash
# Claude Codeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
# å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

# MCP ã‚µãƒ¼ãƒãƒ¼ã‚’è¿½åŠ 
claude mcp add local-llm

# ã¾ãŸã¯æ‰‹å‹•ã§è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†
# ~/.config/claude-code/config.json
```

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### Claude Codeã‹ã‚‰ä½¿ç”¨

```bash
# Claude Codeã‚’èµ·å‹•
claude code

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¾‹:
# "Use local LLM to implement a Python quicksort function"
# "Analyze main.py with local model for potential bugs"
# "Generate a REST API using the local coding model"
```

### åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«

1. **code_with_local_llm**
   - ã‚¿ã‚¹ã‚¯: `"Implement a binary search tree in Python"`
   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå«ã‚€: `true`

2. **read_file_with_analysis**
   - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: `"src/main.py"`
   - åˆ†æã‚¿ã‚¤ãƒ—: `"bugs"` | `"optimization"` | `"documentation"`

3. **write_code_to_file**
   - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: `"utils/helpers.py"`
   - ã‚¿ã‚¹ã‚¯èª¬æ˜: `"Create utility functions for data processing"`

4. **debug_with_llm**
   - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: `"IndexError: list index out of range"`
   - ã‚³ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: è©²å½“ã™ã‚‹ã‚³ãƒ¼ãƒ‰éƒ¨åˆ†

5. **explain_code**
   - ã‚³ãƒ¼ãƒ‰: è§£èª¬ã—ãŸã„ã‚³ãƒ¼ãƒ‰
   - è©³ç´°ãƒ¬ãƒ™ãƒ«: `"basic"` | `"medium"` | `"detailed"`

6. **switch_model**
   - ãƒ¢ãƒ‡ãƒ«å: `"qwen2.5-coder:7b-instruct"`

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ¢ãƒ‡ãƒ«è¨­å®šã®å¤‰æ›´

```python
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›´
llm = LocalLLMServer("deepseek-coder:6.7b-instruct-q4_K_M")

# è¤‡æ•°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
models = {
    "coding": "qwen2.5-coder:14b-instruct-q4_K_M",
    "general": "qwen2.5:14b-instruct-q4_K_M",
    "light": "mistral-nemo:12b-instruct-q5_K_M"
}
```

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª¿æ•´
system_prompt = """You are an expert coding assistant specialized in:
- Clean, efficient code generation
- Best practices and design patterns
- Security-conscious development
- Performance optimization

Always provide:
- Working, tested code
- Comprehensive comments
- Error handling
- Performance considerations"""
```

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

1. **MCPã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ãªã„**
```bash
# ãƒ­ã‚°ç¢ºèª
tail -f ~/.config/claude-desktop/logs/mcp.log

# Pythonãƒ‘ã‚¹ã®ç¢ºèª
which python
```

2. **Ollamaã«æ¥ç¶šã§ããªã„**
```bash
# Ollamaã®çŠ¶æ…‹ç¢ºèª
ollama ps
curl http://localhost:11434/api/tags

# ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
ollama serve
```

3. **ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„**
```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç¢ºèª
ollama list

# ãƒ¢ãƒ‡ãƒ«ã®å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull qwen2.5-coder:14b-instruct-q4_K_M
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

```python
# Ollamaã®è¨­å®šèª¿æ•´
{
    "temperature": 0.1,      # ä¸€è²«æ€§é‡è¦–
    "top_p": 0.95,          # å“è³ªãƒãƒ©ãƒ³ã‚¹
    "num_predict": 2048,    # å¿œç­”é•·åˆ¶é™
    "num_ctx": 4096         # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·
}
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

```python
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™
ALLOWED_DIRECTORIES = [
    os.getcwd(),
    os.path.expanduser("~/projects")
]

# å®Ÿè¡Œå¯èƒ½ã‚³ãƒãƒ³ãƒ‰ã®åˆ¶é™
ALLOWED_COMMANDS = ["git", "python", "node", "npm"]
```

## ğŸ‰ ä½¿ç”¨ä¾‹

### 1. æ–°æ©Ÿèƒ½ã®å®Ÿè£…
```
Claude Code Prompt:
"Use local LLM to create a user authentication system with JWT tokens in Python Flask"

â†’ MCPã‚µãƒ¼ãƒãƒ¼ãŒãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
â†’ ãƒ•ã‚¡ã‚¤ãƒ«ã«è‡ªå‹•ä¿å­˜
â†’ Claude CodeãŒçµæœã‚’è¡¨ç¤º
```

### 2. ãƒã‚°ä¿®æ­£
```
Claude Code Prompt:
"Analyze app.py for bugs and fix them using the local model"

â†’ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ + LLMåˆ†æ
â†’ ä¿®æ­£ç‰ˆã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
â†’ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå¾Œã«ä¸Šæ›¸ã
```

### 3. ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼
```
Claude Code Prompt:
"Review the entire codebase with local LLM and provide optimization suggestions"

â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’ã‚¹ã‚­ãƒ£ãƒ³
â†’ å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’LLMã§åˆ†æ
â†’ æ”¹å–„ææ¡ˆã‚’ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã§ç”Ÿæˆ
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| æ©Ÿèƒ½ | Claude Code (å…¬å¼) | ãƒ­ãƒ¼ã‚«ãƒ«LLM + MCP |
|------|-------------------|-------------------|
| å¿œç­”é€Ÿåº¦ | âš¡ é«˜é€Ÿ | ğŸŸ¡ ä¸­ç¨‹åº¦ |
| ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ | ğŸŸ¡ ã‚¯ãƒ©ã‚¦ãƒ‰ | ğŸŸ¢ å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ« |
| ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º | ğŸŸ¡ é™å®šçš„ | ğŸŸ¢ å®Œå…¨è‡ªç”± |
| ã‚³ã‚¹ãƒˆ | ğŸ’° å¾“é‡èª²é‡‘ | ğŸŸ¢ ç„¡æ–™ |
| å°‚é–€æ€§ | ğŸŸ¢ æ±ç”¨çš„ | ğŸŸ¢ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ |

## ğŸ”„ ä»Šå¾Œã®æ‹¡å¼µ

- [ ] è¤‡æ•°LLMãƒ¢ãƒ‡ãƒ«ã®åŒæ™‚åˆ©ç”¨
- [ ] ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œç’°å¢ƒã®çµ±åˆ
- [ ] Gitãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è‡ªå‹•åŒ–
- [ ] ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ç”Ÿæˆ
- [ ] è‡ªå‹•ãƒ†ã‚¹ãƒˆç”Ÿæˆæ©Ÿèƒ½