#!/usr/bin/env python3
"""
Local LLM MCP Server for Claude Code Integration
Claude Code â†’ MCP Server â†’ Local LLM (Qwen2.5-Coder)
"""

import asyncio
import json
import logging
import requests
import subprocess
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from mcp.server import Server
from mcp.types import (
    Tool, 
    TextContent, 
    Resource,
    PromptMessage,
    GetPromptResult
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("local-llm-mcp")

class LocalLLMServer:
    def __init__(self, model: str = "qwen2.5-coder:14b-instruct-q4_K_M"):
        self.model = model
        self.ollama_url = "http://localhost:11434"
        self.conversation_history = []
        
    def call_ollama(self, prompt: str, system_prompt: str = "") -> str:
        """Ollamaã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡"""
        try:
            full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.95,
                        "num_predict": 2048,
                        "stop": ["User:", "Human:"]
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()["response"].strip()
            else:
                return f"Error: {response.status_code} - {response.text}"
                
        except Exception as e:
            logger.error(f"Ollama call failed: {e}")
            return f"Connection error: {e}"
    
    def get_project_context(self) -> str:
        """ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æƒ…å ±ã‚’å–å¾—"""
        context = []
        
        # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        cwd = os.getcwd()
        context.append(f"Current directory: {cwd}")
        
        # Gitæƒ…å ±
        try:
            git_status = subprocess.run(
                ["git", "status", "--porcelain"], 
                capture_output=True, text=True, cwd=cwd
            )
            if git_status.returncode == 0:
                context.append(f"Git status: {git_status.stdout.strip() or 'Clean'}")
        except:
            context.append("Git: Not a git repository")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        try:
            files = []
            for item in Path(cwd).iterdir():
                if not item.name.startswith('.') and item.name not in ['node_modules', '__pycache__']:
                    if item.is_file():
                        files.append(f"ğŸ“„ {item.name}")
                    elif item.is_dir():
                        files.append(f"ğŸ“ {item.name}/")
            
            if files:
                context.append("Project files:")
                context.extend(files[:10])  # æœ€åˆã®10å€‹ã¾ã§
                
        except Exception as e:
            context.append(f"File listing error: {e}")
        
        return "\n".join(context)

# MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
app = Server("local-llm-mcp")
llm = LocalLLMServer()

@app.tool("code_with_local_llm")
async def code_with_local_llm(
    task: str,
    include_context: bool = True,
    model_override: str = ""
) -> str:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
    
    Args:
        task: å®Ÿè¡Œã—ãŸã„ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯
        include_context: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã‚‹ã‹
        model_override: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ™‚çš„ã«å¤‰æ›´
    """
    logger.info(f"Executing coding task: {task}")
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¸€æ™‚å¤‰æ›´
    original_model = llm.model
    if model_override:
        llm.model = model_override
    
    try:
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        system_prompt = """You are an expert coding assistant. You can:
1. Write, analyze, and debug code
2. Explain programming concepts
3. Suggest optimizations and best practices
4. Generate complete, working solutions

Always provide:
- Clear, commented code
- Explanations of your approach
- Any assumptions you've made
- Suggestions for improvements

Format your response clearly with code blocks and explanations."""

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
        if include_context:
            context = llm.get_project_context()
            system_prompt += f"\n\nCurrent project context:\n{context}"
        
        # LLMã«é€ä¿¡
        response = llm.call_ollama(task, system_prompt)
        
        return response
        
    except Exception as e:
        logger.error(f"Code generation failed: {e}")
        return f"âŒ Error in code generation: {e}"
    finally:
        # ãƒ¢ãƒ‡ãƒ«ã‚’å…ƒã«æˆ»ã™
        llm.model = original_model

@app.tool("read_file_with_analysis")
async def read_file_with_analysis(
    filepath: str,
    analysis_type: str = "general"
) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§LLMã§åˆ†æ
    
    Args:
        filepath: åˆ†æã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        analysis_type: åˆ†æã‚¿ã‚¤ãƒ— (general, bugs, optimization, documentation)
    """
    logger.info(f"Analyzing file: {filepath}")
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # åˆ†æã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        analysis_prompts = {
            "general": "Analyze this code and provide a general overview, including its purpose, structure, and key components.",
            "bugs": "Review this code for potential bugs, errors, or issues. Suggest fixes if found.",
            "optimization": "Analyze this code for performance optimizations and suggest improvements.",
            "documentation": "Generate comprehensive documentation for this code, including docstrings and comments."
        }
        
        prompt = f"{analysis_prompts.get(analysis_type, analysis_prompts['general'])}\n\nFile: {filepath}\n\nCode:\n```\n{content}\n```"
        
        system_prompt = "You are a code review expert. Provide detailed, constructive analysis."
        
        response = llm.call_ollama(prompt, system_prompt)
        
        return f"ğŸ“‹ Analysis of {filepath}:\n\n{response}"
        
    except FileNotFoundError:
        return f"âŒ File not found: {filepath}"
    except Exception as e:
        logger.error(f"File analysis failed: {e}")
        return f"âŒ Error analyzing file: {e}"

@app.tool("write_code_to_file")
async def write_code_to_file(
    filepath: str,
    task_description: str,
    overwrite: bool = False
) -> str:
    """
    LLMã§ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    
    Args:
        filepath: æ›¸ãè¾¼ã¿å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        task_description: ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®ã‚¿ã‚¹ã‚¯èª¬æ˜
        overwrite: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã™ã‚‹ã‹
    """
    logger.info(f"Generating code for file: {filepath}")
    
    try:
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        if os.path.exists(filepath) and not overwrite:
            return f"âŒ File already exists: {filepath}. Use overwrite=true to replace."
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰è¨€èªã‚’æ¨å®š
        ext = Path(filepath).suffix.lower()
        language_map = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.ts': 'TypeScript',
            '.java': 'Java',
            '.cpp': 'C++',
            '.c': 'C',
            '.rs': 'Rust',
            '.go': 'Go'
        }
        language = language_map.get(ext, 'appropriate language')
        
        # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""Generate {language} code for the following task and save it to {filepath}:

Task: {task_description}

Requirements:
- Write complete, working code
- Include appropriate comments
- Follow best practices for {language}
- Make the code production-ready

Return ONLY the code that should be saved to the file, without any additional explanation or markdown formatting."""
        
        system_prompt = f"You are an expert {language} developer. Generate clean, efficient, well-documented code."
        
        # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
        generated_code = llm.call_ollama(prompt, system_prompt)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(generated_code)
        
        return f"âœ… Code generated and saved to {filepath}\n\nGenerated code:\n```{language.lower()}\n{generated_code}\n```"
        
    except Exception as e:
        logger.error(f"Code generation and file writing failed: {e}")
        return f"âŒ Error: {e}"

@app.tool("debug_with_llm")
async def debug_with_llm(
    error_message: str,
    code_context: str = "",
    filepath: str = ""
) -> str:
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã‚³ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ‡ãƒãƒƒã‚°æ”¯æ´
    
    Args:
        error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        code_context: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚³ãƒ¼ãƒ‰ã®éƒ¨åˆ†
        filepath: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    logger.info("Debugging with LLM")
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°èª­ã¿è¾¼ã¿
        if filepath and os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                file_content = f.read()
            code_context = f"Full file content:\n{file_content}"
        
        prompt = f"""Help debug this error:

Error message: {error_message}

Code context:
{code_context}

Please:
1. Explain what's causing the error
2. Provide a specific solution
3. Show the corrected code if applicable
4. Suggest ways to prevent similar errors"""
        
        system_prompt = "You are an expert debugger. Provide clear, actionable solutions to programming errors."
        
        response = llm.call_ollama(prompt, system_prompt)
        
        return f"ğŸ”§ Debug Analysis:\n\n{response}"
        
    except Exception as e:
        logger.error(f"Debugging failed: {e}")
        return f"âŒ Debug error: {e}"

@app.tool("explain_code")
async def explain_code(
    code: str,
    detail_level: str = "medium"
) -> str:
    """
    ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜ã‚’ç”Ÿæˆ
    
    Args:
        code: èª¬æ˜ã™ã‚‹ã‚³ãƒ¼ãƒ‰
        detail_level: èª¬æ˜ã®è©³ç´°ãƒ¬ãƒ™ãƒ« (basic, medium, detailed)
    """
    logger.info("Explaining code with LLM")
    
    try:
        detail_prompts = {
            "basic": "Provide a brief, high-level explanation of what this code does.",
            "medium": "Explain this code in detail, including its purpose, how it works, and key components.",
            "detailed": "Provide a comprehensive explanation including line-by-line analysis, design patterns used, and potential improvements."
        }
        
        prompt = f"{detail_prompts.get(detail_level, detail_prompts['medium'])}\n\nCode:\n```\n{code}\n```"
        
        system_prompt = "You are a programming instructor. Explain code clearly and educationally."
        
        response = llm.call_ollama(prompt, system_prompt)
        
        return f"ğŸ“š Code Explanation:\n\n{response}"
        
    except Exception as e:
        logger.error(f"Code explanation failed: {e}")
        return f"âŒ Explanation error: {e}"

@app.tool("switch_model")
async def switch_model(model_name: str) -> str:
    """
    ä½¿ç”¨ã™ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ
    
    Args:
        model_name: åˆ‡ã‚Šæ›¿ãˆå…ˆã®ãƒ¢ãƒ‡ãƒ«å
    """
    logger.info(f"Switching model to: {model_name}")
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã®å­˜åœ¨ç¢ºèª
        response = requests.get(f"{llm.ollama_url}/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            available_models = [model["name"] for model in models]
            
            if model_name in available_models:
                llm.model = model_name
                return f"âœ… Model switched to: {model_name}"
            else:
                return f"âŒ Model not found. Available models: {', '.join(available_models)}"
        else:
            return "âŒ Cannot check available models"
            
    except Exception as e:
        logger.error(f"Model switching failed: {e}")
        return f"âŒ Error switching model: {e}"

async def main():
    """MCPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
    logger.info("Starting Local LLM MCP Server...")
    logger.info(f"Using model: {llm.model}")
    
    # Ollamaã®æ¥ç¶šç¢ºèª
    try:
        response = requests.get(f"{llm.ollama_url}/api/tags", timeout=5)
        if response.status_code == 200:
            logger.info("âœ… Ollama connection successful")
        else:
            logger.warning("âš ï¸ Ollama connection issue")
    except Exception as e:
        logger.error(f"âŒ Cannot connect to Ollama: {e}")
    
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    await app.run()

if __name__ == "__main__":
    asyncio.run(main())