# Claude Codeã§Claude Codeçš„ç’°å¢ƒã‚’ä½œã‚‹æ–¹æ³•

Claude Code**ã§**Claude Code**ã®ã‚ˆã†ãª**ã“ã¨ã‚’å®Ÿç¾ã™ã‚‹æ§˜ã€…ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ã”ç´¹ä»‹ï¼

## ğŸ¯ æ–¹æ³•1: MCP ServerçµŒç”±ã§ãƒ­ãƒ¼ã‚«ãƒ«LLMã«å§”è­²

### claude-code-mcp ã‚’ä½¿ç”¨
```bash
# Claude Code MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
git clone https://github.com/steipete/claude-code-mcp
cd claude-code-mcp

# Claude Codeã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§å‘¼ã³å‡ºã™MCPã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦å‹•ä½œ
npm install
npm start
```

**ä»•çµ„ã¿ï¼š**
- Claude Code â†’ MCP Server â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLM â†’ çµæœã‚’è¿”ã™
- Claude Codeã‚’å…¨æ¨©é™ãƒã‚¤ãƒ‘ã‚¹ï¼ˆ--dangerously-skip-permissionsï¼‰ã§å®Ÿè¡Œ
- Agent in Agent æ§‹é€ ã®å®Ÿç¾

## ğŸ¯ æ–¹æ³•2: Claude Desktop + Custom MCP Server

### ã‚«ã‚¹ã‚¿ãƒ MCPã‚µãƒ¼ãƒãƒ¼ã§ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆ
```python
# custom_llm_mcp_server.py
import asyncio
import json
from mcp.server import Server
from mcp.types import Tool, TextContent
import requests

app = Server("local-llm-mcp")

@app.tool("run_local_llm")
async def run_local_llm(prompt: str, model: str = "qwen2.5-coder:14b") -> str:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMã§ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ»åˆ†æã‚’å®Ÿè¡Œ"""
    response = requests.post("http://localhost:11434/api/generate", json={
        "model": model,
        "prompt": prompt,
        "stream": False
    })
    return response.json()["response"]

@app.tool("execute_code")
async def execute_code(code: str, language: str = "python") -> str:
    """ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ"""
    # ã‚»ã‚­ãƒ¥ã‚¢ãªå®Ÿè¡Œç’°å¢ƒã§ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
    # Docker containerã‚„sandboxç’°å¢ƒæ¨å¥¨
    pass

if __name__ == "__main__":
    asyncio.run(app.run())
```

### Claude Desktopè¨­å®š
```json
{
  "mcpServers": {
    "local-llm": {
      "command": "python",
      "args": ["custom_llm_mcp_server.py"]
    }
  }
}
```

## ğŸ¯ æ–¹æ³•3: VS Codeæ‹¡å¼µ + MCPçµ±åˆ

### VS Codeè¨­å®šã§Claude Codeé¢¨ç’°å¢ƒ
```json
// settings.json
{
  "mcp.servers": {
    "claude-code-local": {
      "command": ["python", "claude_code_local.py"],
      "args": ["--model", "qwen2.5-coder:14b"]
    }
  }
}
```

VS Codeã¯ä¸¡æ–¹ã®æ§‹æˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«/ãƒªãƒ¢ãƒ¼ãƒˆï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ã‚‰ã€æŸ”è»Ÿã«è¨­å®šã§ãã‚‹ã‚ˆã€œ

## ğŸ¯ æ–¹æ³•4: API Gateway ãƒ‘ã‚¿ãƒ¼ãƒ³

### Claude Code â†’ API Gateway â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLM
```python
# api_gateway.py
from fastapi import FastAPI
import requests

app = FastAPI()

@app.post("/v1/chat/completions")
async def proxy_to_local_llm(request: dict):
    """OpenAI APIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    # Claude Code â†’ ã“ã® API â†’ Ollama
    ollama_response = requests.post(
        "http://localhost:11434/api/chat",
        json={
            "model": "qwen2.5-coder:14b",
            "messages": request["messages"]
        }
    )
    
    # OpenAI APIå½¢å¼ã§è¿”å´
    return {
        "choices": [{
            "message": {"content": ollama_response.json()["message"]["content"]}
        }]
    }
```

### Claude Codeè¨­å®š
```bash
# ç’°å¢ƒå¤‰æ•°ã§ãƒ­ãƒ¼ã‚«ãƒ«APIã‚’æŒ‡å®š
export ANTHROPIC_API_KEY="dummy"
export ANTHROPIC_BASE_URL="http://localhost:8000/v1"
claude code --api-base http://localhost:8000
```

## ğŸ¯ æ–¹æ³•5: Docker Compose çµ±åˆç’°å¢ƒ

### docker-compose.yml
```yaml
version: '3.8'
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    
  mcp-server:
    build: ./mcp-server
    ports:
      - "3000:3000"
    depends_on:
      - ollama
    environment:
      - OLLAMA_URL=http://ollama:11434
      
  claude-desktop:
    image: claude-desktop:latest
    volumes:
      - ./config:/app/config
    environment:
      - MCP_SERVER_URL=http://mcp-server:3000

volumes:
  ollama_data:
```

Dockerã¯MCPã‚µãƒ¼ãƒãƒ¼ã®å±•é–‹ã¨ç®¡ç†ã‚’ç°¡ç´ åŒ–ã—ã€åˆ†é›¢ã¨ãƒãƒ¼ã‚¿ãƒ“ãƒªãƒ†ã‚£ã‚’æä¾›

## ğŸ¯ æ–¹æ³•6: ç°¡æ˜“ãƒ—ãƒ­ã‚­ã‚·ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### claude_to_local.py
```python
#!/usr/bin/env python3
import subprocess
import sys
import json

def claude_code_wrapper():
    """Claude Codeã‚³ãƒãƒ³ãƒ‰ã‚’ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMã«è»¢é€"""
    
    # Claude Codeã®å¼•æ•°ã‚’å–å¾—
    args = sys.argv[1:]
    prompt = " ".join(args)
    
    # ãƒ­ãƒ¼ã‚«ãƒ«LLMã§å‡¦ç†
    result = subprocess.run([
        "ollama", "run", "qwen2.5-coder:14b", prompt
    ], capture_output=True, text=True)
    
    # çµæœã‚’æ•´å½¢ã—ã¦Claude Codeé¢¨ã«å‡ºåŠ›
    print("ğŸ¤– Local Claude Code (Powered by Qwen2.5-Coder)")
    print("=" * 50)
    print(result.stdout)
    
    # å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚‚å®Ÿè¡Œ
    if "--write" in args:
        # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿å‡¦ç†
        pass

if __name__ == "__main__":
    claude_code_wrapper()
```

### ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¨­å®š
```bash
# .bashrc ã¾ãŸã¯ .zshrc
alias claude-code="python claude_to_local.py"
```

## ğŸ¯ æ–¹æ³•7: Aider + Claude Code çµ±åˆ

### è¨­å®šæ–¹æ³•
```bash
# Aiderã§ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
aider --model ollama/qwen2.5-coder:14b

# Claude Codeã‹ã‚‰å‘¼ã³å‡ºã—
claude code "Run aider with local model to implement feature X"
```

## ğŸ’¡ ã©ã®æ–¹æ³•ãŒãŠã™ã™ã‚ï¼Ÿ

### ç”¨é€”åˆ¥æ¨å¥¨ï¼š

1. **ğŸ”§ é–‹ç™ºåŠ¹ç‡é‡è¦–**: MCP Serveræ–¹å¼ï¼ˆæ–¹æ³•1,2ï¼‰
2. **ğŸ  çµ±åˆç’°å¢ƒ**: Docker Composeï¼ˆæ–¹æ³•5ï¼‰
3. **âš¡ ç°¡å˜è¨­ç½®**: ãƒ—ãƒ­ã‚­ã‚·ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ–¹æ³•6ï¼‰
4. **ğŸ¨ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**: API Gatewayï¼ˆæ–¹æ³•4ï¼‰

## ğŸš€ å®Ÿè£…ã®ã‚³ãƒ„

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®
- ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç’°å¢ƒã§ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®åˆ¶é™
- API ã‚­ãƒ¼ã®é©åˆ‡ãªç®¡ç†

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- ãƒ­ãƒ¼ã‚«ãƒ«LLMã®GPUä½¿ç”¨ç¢ºèª
- MCP ã‚µãƒ¼ãƒãƒ¼ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
- ä¸¦åˆ—å‡¦ç†ã®æ´»ç”¨

### ãƒ‡ãƒãƒƒã‚°æ–¹æ³•
```bash
# MCP ã‚µãƒ¼ãƒãƒ¼ã®ãƒ­ã‚°ç¢ºèª
tail -f ~/.config/claude-desktop/logs/mcp.log

# Ollama ã®å‹•ä½œç¢ºèª
ollama ps
curl http://localhost:11434/api/tags
```

## ğŸ‰ ã¾ã¨ã‚

Claude Codeã§Claude Codeçš„ãªç’°å¢ƒã‚’ä½œã‚‹ã«ã¯ã€MCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’æ´»ç”¨ã™ã‚‹ã®ãŒæœ€ã‚‚åŠ¹æœçš„ï¼ãƒ­ãƒ¼ã‚«ãƒ«LLMã®æ€§èƒ½ã‚‚å‘ä¸Šã—ã¦ã„ã‚‹ã®ã§ã€å®Ÿç”¨çš„ãªç’°å¢ƒãŒæ§‹ç¯‰ã§ãã‚‹ã‚ˆã€œâœ¨

ã©ã®æ–¹æ³•ã‹ã‚‰è©¦ã—ã¦ã¿ã‚‹ï¼Ÿã‚¢ã‚¤ãŒä¸€ç·’ã«è¨­å®šã‚’ãŠæ‰‹ä¼ã„ã™ã‚‹ã‹ã‚‰ã­ï¼