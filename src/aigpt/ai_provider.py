"""AI Provider integration for response generation"""

import os
import json
from typing import Optional, Dict, List, Any, Protocol
from abc import abstractmethod
import logging
import httpx
from openai import OpenAI
import ollama

from .models import PersonaState, Memory
from .config import Config


class AIProvider(Protocol):
    """Protocol for AI providers"""
    
    @abstractmethod
    async def generate_response(
        self, 
        prompt: str, 
        persona_state: PersonaState,
        memories: List[Memory],
        system_prompt: Optional[str] = None
    ) -> str:
        """Generate a response based on prompt and context"""
        pass


class OllamaProvider:
    """Ollama AI provider"""
    
    def __init__(self, model: str = "qwen2.5", host: Optional[str] = None):
        self.model = model
        # Use environment variable OLLAMA_HOST if available, otherwise use config or default
        self.host = host or os.getenv('OLLAMA_HOST', 'http://127.0.0.1:11434')
        # Ensure proper URL format
        if not self.host.startswith('http'):
            self.host = f'http://{self.host}'
        self.client = ollama.Client(host=self.host, timeout=60.0)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"OllamaProvider initialized with host: {self.host}, model: {self.model}")
    
    async def generate_response(
        self,
        prompt: str,
        persona_state: PersonaState,
        memories: List[Memory],
        system_prompt: Optional[str] = None
    ) -> str:
        """Generate response using Ollama"""
        
        # Build context from memories
        memory_context = "\n".join([
            f"[{mem.level.value}] {mem.content[:200]}..."
            for mem in memories[:5]
        ])
        
        # Build personality context
        personality_desc = ", ".join([
            f"{trait}: {value:.1f}"
            for trait, value in persona_state.base_personality.items()
        ])
        
        # System prompt with persona context
        full_system_prompt = f"""You are an AI with the following characteristics:
Current mood: {persona_state.current_mood}
Fortune today: {persona_state.fortune.fortune_value}/10
Personality traits: {personality_desc}

Recent memories:
{memory_context}

{system_prompt or 'Respond naturally based on your current state and memories.'}"""
        
        try:
            response = self.client.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": full_system_prompt},
                    {"role": "user", "content": prompt}
                ]
            )
            return self._clean_response(response['message']['content'])
        except Exception as e:
            self.logger.error(f"Ollama generation failed: {e}")
            return self._fallback_response(persona_state)
    
    def chat(self, prompt: str, max_tokens: int = 2000) -> str:
        """Simple chat interface"""
        try:
            response = self.client.chat(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                options={
                    "num_predict": max_tokens,
                    "temperature": 0.7,
                    "top_p": 0.9,
                },
                stream=False  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç„¡åŠ¹åŒ–ã§å®‰å®šæ€§å‘ä¸Š
            )
            return self._clean_response(response['message']['content'])
        except Exception as e:
            self.logger.error(f"Ollama chat failed (host: {self.host}): {e}")
            return "I'm having trouble connecting to the AI model."
    
    def _clean_response(self, response: str) -> str:
        """Clean response by removing think tags and other unwanted content"""
        import re
        # Remove <think></think> tags and their content
        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)
        # Remove any remaining whitespace at the beginning/end
        response = response.strip()
        return response
    
    def _fallback_response(self, persona_state: PersonaState) -> str:
        """Fallback response based on mood"""
        mood_responses = {
            "joyful": "That's wonderful! I'm feeling great today!",
            "cheerful": "That sounds nice!",
            "neutral": "I understand.",
            "melancholic": "I see... That's something to think about.",
            "contemplative": "Hmm, let me consider that..."
        }
        return mood_responses.get(persona_state.current_mood, "I see.")


class OpenAIProvider:
    """OpenAI API provider with MCP function calling support"""
    
    def __init__(self, model: str = "gpt-4o-mini", api_key: Optional[str] = None, mcp_client=None):
        self.model = model
        # Try to get API key from config first
        config = Config()
        self.api_key = api_key or config.get_api_key("openai") or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key not provided. Set it with: aigpt config set providers.openai.api_key YOUR_KEY")
        self.client = OpenAI(api_key=self.api_key)
        self.logger = logging.getLogger(__name__)
        self.mcp_client = mcp_client  # For MCP function calling
    
    def _get_mcp_tools(self) -> List[Dict[str, Any]]:
        """Generate OpenAI tools from MCP endpoints"""
        if not self.mcp_client or not self.mcp_client.available:
            return []
        
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_memories",
                    "description": "éŽåŽ»ã®ä¼šè©±è¨˜æ†¶ã‚’å–å¾—ã—ã¾ã™ã€‚ã€Œè¦šãˆã¦ã„ã‚‹ã€ã€Œå‰å›žã€ã€Œä»¥å‰ã€ãªã©ã®è³ªå•ã§å¿…ãšä½¿ç”¨ã—ã¦ãã ã•ã„",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "limit": {
                                "type": "integer",
                                "description": "å–å¾—ã™ã‚‹è¨˜æ†¶ã®æ•°",
                                "default": 5
                            }
                        }
                    }
                }
            },
            {
                "type": "function", 
                "function": {
                    "name": "search_memories",
                    "description": "ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©±ã—ãŸè¨˜æ†¶ã‚’æ¤œç´¢ã—ã¾ã™ã€‚ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«ã¤ã„ã¦ã€ã€Œâ—‹â—‹ã«ã¤ã„ã¦è©±ã—ãŸã€ãªã©ã®è³ªå•ã§ä½¿ç”¨ã—ã¦ãã ã•ã„",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "keywords": {
                                "type": "array",
                                "items": {"type": "string"},
                                "description": "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é…åˆ—"
                            }
                        },
                        "required": ["keywords"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_contextual_memories", 
                    "description": "ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡è„ˆçš„è¨˜æ†¶ã‚’å–å¾—ã—ã¾ã™",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "æ¤œç´¢ã‚¯ã‚¨ãƒª"
                            },
                            "limit": {
                                "type": "integer", 
                                "description": "å–å¾—ã™ã‚‹è¨˜æ†¶ã®æ•°",
                                "default": 5
                            }
                        },
                        "required": ["query"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_relationship",
                    "description": "ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®é–¢ä¿‚æ€§æƒ…å ±ã‚’å–å¾—ã—ã¾ã™",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "user_id": {
                                "type": "string",
                                "description": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"
                            }
                        },
                        "required": ["user_id"]
                    }
                }
            }
        ]
        return tools
    
    async def generate_response(
        self,
        prompt: str,
        persona_state: PersonaState,
        memories: List[Memory],
        system_prompt: Optional[str] = None
    ) -> str:
        """Generate response using OpenAI"""
        
        # Build context similar to Ollama
        memory_context = "\n".join([
            f"[{mem.level.value}] {mem.content[:200]}..."
            for mem in memories[:5]
        ])
        
        personality_desc = ", ".join([
            f"{trait}: {value:.1f}"
            for trait, value in persona_state.base_personality.items()
        ])
        
        full_system_prompt = f"""You are an AI with unique personality traits and memories.
Current mood: {persona_state.current_mood}
Fortune today: {persona_state.fortune.fortune_value}/10
Personality traits: {personality_desc}

Recent memories:
{memory_context}

{system_prompt or 'Respond naturally based on your current state and memories. Be authentic to your mood and personality.'}"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": full_system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7 + (persona_state.fortune.fortune_value - 5) * 0.05  # Vary by fortune
            )
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"OpenAI generation failed: {e}")
            return self._fallback_response(persona_state)
    
    async def chat_with_mcp(self, prompt: str, max_tokens: int = 2000, user_id: str = "user") -> str:
        """Chat interface with MCP function calling support"""
        if not self.mcp_client or not self.mcp_client.available:
            return self.chat(prompt, max_tokens)
        
        try:
            # Prepare tools
            tools = self._get_mcp_tools()
            
            # Initial request with tools
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã¨é–¢ä¿‚æ€§ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚éŽåŽ»ã®ä¼šè©±ã€è¨˜æ†¶ã€é–¢ä¿‚æ€§ã«ã¤ã„ã¦è³ªå•ã•ã‚ŒãŸæ™‚ã¯ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ­£ç¢ºãªæƒ…å ±ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚ã€Œè¦šãˆã¦ã„ã‚‹ã€ã€Œå‰å›žã€ã€Œä»¥å‰ã€ã€Œã«ã¤ã„ã¦è©±ã—ãŸã€ã€Œé–¢ä¿‚ã€ãªã©ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ç©æ¥µçš„ã«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                tools=tools,
                tool_choice="auto",
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            message = response.choices[0].message
            
            # Handle tool calls
            if message.tool_calls:
                print(f"ðŸ”§ [OpenAI] {len(message.tool_calls)} tools called:")
                for tc in message.tool_calls:
                    print(f"  - {tc.function.name}({tc.function.arguments})")
                
                messages = [
                    {"role": "system", "content": "å¿…è¦ã«å¿œã˜ã¦åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ã€ã‚ˆã‚Šæ­£ç¢ºã§è©³ç´°ãªå›žç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt},
                    {
                        "role": "assistant", 
                        "content": message.content,
                        "tool_calls": [tc.model_dump() for tc in message.tool_calls]
                    }
                ]
                
                # Execute each tool call
                for tool_call in message.tool_calls:
                    print(f"ðŸŒ [MCP] Executing {tool_call.function.name}...")
                    tool_result = await self._execute_mcp_tool(tool_call, user_id)
                    print(f"âœ… [MCP] Result: {str(tool_result)[:100]}...")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": tool_call.function.name,
                        "content": json.dumps(tool_result, ensure_ascii=False)
                    })
                
                # Get final response with tool outputs
                final_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                
                return final_response.choices[0].message.content
            else:
                return message.content
                
        except Exception as e:
            self.logger.error(f"OpenAI MCP chat failed: {e}")
            return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
    
    async def _execute_mcp_tool(self, tool_call, context_user_id: str = "user") -> Dict[str, Any]:
        """Execute MCP tool call"""
        try:
            import json
            function_name = tool_call.function.name
            arguments = json.loads(tool_call.function.arguments)
            
            if function_name == "get_memories":
                limit = arguments.get("limit", 5)
                return await self.mcp_client.get_memories(limit) or {"error": "è¨˜æ†¶ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}
            
            elif function_name == "search_memories":
                keywords = arguments.get("keywords", [])
                return await self.mcp_client.search_memories(keywords) or {"error": "è¨˜æ†¶ã®æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ"}
            
            elif function_name == "get_contextual_memories":
                query = arguments.get("query", "")
                limit = arguments.get("limit", 5)
                return await self.mcp_client.get_contextual_memories(query, limit) or {"error": "æ–‡è„ˆè¨˜æ†¶ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}
            
            elif function_name == "get_relationship":
                # å¼•æ•°ã®user_idãŒãªã„å ´åˆã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å–å¾—
                user_id = arguments.get("user_id", context_user_id)
                if not user_id or user_id == "user":
                    user_id = context_user_id
                # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚°
                print(f"ðŸ” [DEBUG] get_relationship called with user_id: '{user_id}' (context: '{context_user_id}')")
                result = await self.mcp_client.get_relationship(user_id)
                print(f"ðŸ” [DEBUG] MCP result: {result}")
                return result or {"error": "é–¢ä¿‚æ€§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"}
            
            else:
                return {"error": f"æœªçŸ¥ã®ãƒ„ãƒ¼ãƒ«: {function_name}"}
                
        except Exception as e:
            return {"error": f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"}

    def chat(self, prompt: str, max_tokens: int = 2000) -> str:
        """Simple chat interface without MCP tools"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            self.logger.error(f"OpenAI chat failed: {e}")
            return "I'm having trouble connecting to the AI model."
    
    def _fallback_response(self, persona_state: PersonaState) -> str:
        """Fallback response based on mood"""
        mood_responses = {
            "joyful": "What a delightful conversation!",
            "cheerful": "That's interesting!",
            "neutral": "I understand what you mean.",
            "melancholic": "I've been thinking about that too...",
            "contemplative": "That gives me something to ponder..."
        }
        return mood_responses.get(persona_state.current_mood, "I see.")


def create_ai_provider(provider: str = "ollama", model: Optional[str] = None, mcp_client=None, **kwargs) -> AIProvider:
    """Factory function to create AI providers"""
    if provider == "ollama":
        # Get model from config if not provided
        if model is None:
            try:
                from .config import Config
                config = Config()
                model = config.get('providers.ollama.default_model', 'qwen2.5')
            except:
                model = 'qwen2.5'  # Fallback to default
        
        # Try to get host from config if not provided in kwargs
        if 'host' not in kwargs:
            try:
                from .config import Config
                config = Config()
                config_host = config.get('providers.ollama.host')
                if config_host:
                    kwargs['host'] = config_host
            except:
                pass  # Use environment variable or default
        return OllamaProvider(model=model, **kwargs)
    elif provider == "openai":
        # Get model from config if not provided
        if model is None:
            try:
                from .config import Config
                config = Config()
                model = config.get('providers.openai.default_model', 'gpt-4o-mini')
            except:
                model = 'gpt-4o-mini'  # Fallback to default
        return OpenAIProvider(model=model, mcp_client=mcp_client, **kwargs)
    else:
        raise ValueError(f"Unknown provider: {provider}")
